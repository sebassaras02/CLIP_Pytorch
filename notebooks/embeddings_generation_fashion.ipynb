{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import load\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clip/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTModel, AutoModelForMaskedLM, AutoTokenizer, ViTImageProcessor, DistilBertModel\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "\n",
    "load_dotenv('../.env')\n",
    "pc = Pinecone()\n",
    "index = pc.Index(\"clipmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/t1/1g6kqykj7pjbt9l4nwcplwz00000gn/T/ipykernel_18755/1857697052.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  clip_model.load_state_dict(torch.load('/Users/sebastianalejandrosarastizambonino/Documents/projects/CLIP_Pytorch/src/best_model_fashion.pth', map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from model import CLIPChemistryModel, TextEncoderHead, ImageEncoderHead\n",
    "\n",
    "\n",
    "ENCODER_BASE = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "IMAGE_BASE = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "text_encoder = TextEncoderHead(model=ENCODER_BASE)\n",
    "image_encoder = ImageEncoderHead(model=IMAGE_BASE)\n",
    "\n",
    "clip_model = CLIPChemistryModel(text_encoder=text_encoder, image_encoder=image_encoder)\n",
    "\n",
    "clip_model.load_state_dict(torch.load('/Users/sebastianalejandrosarastizambonino/Documents/projects/CLIP_Pytorch/src/best_model_fashion.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_final = clip_model.text_encoder\n",
    "ie_final = clip_model.image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_for_encoder(text, model):\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=256)\n",
    "    input_ids = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    return output.detach().numpy().tolist()[0]\n",
    "\n",
    "def process_image_for_encoder(image, model):\n",
    "    image = Image.open(BytesIO(image))\n",
    "    # print(type(image))\n",
    "    image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "    image_tensor = image_processor(image, \n",
    "            return_tensors=\"pt\", \n",
    "            do_resize=True\n",
    "            )['pixel_values']\n",
    "    output =  model(pixel_values=image_tensor)\n",
    "    return output.detach().numpy().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset = \"hf://datasets/rajuptvs/ecommerce_products_clip/data/train-00000-of-00001-1f042f20fd269c32.parquet\"\n",
    "df = pd.read_parquet(fashion_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = []\n",
    "for row in df.iterrows():\n",
    "    output = process_text_for_encoder(row[1]['Clipinfo'], te_final)\n",
    "    text_embeddings.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = []\n",
    "for row in df.iterrows():\n",
    "    output = process_image_for_encoder(row[1]['image']['bytes'], ie_final)\n",
    "    image_embeddings.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def bytes_to_str(bytes_data):\n",
    "    return base64.b64encode(bytes_data).decode('utf-8')\n",
    "\n",
    "def str_to_bytes(str_data):\n",
    "    return base64.b64decode(str_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def compress_image(image_bytes, quality=5):\n",
    "    img = Image.open(io.BytesIO(image_bytes))\n",
    "    buffer = io.BytesIO()\n",
    "    img.save(buffer, format='JPEG', quality=quality)\n",
    "    return buffer.getvalue()\n",
    "\n",
    "def push_embeddings_to_pine_cone(index, embeddings, df, mode, length, batch_size=50):\n",
    "    \"\"\"\n",
    "    Push embeddings to Pinecone in batches to avoid message size limits\n",
    "    \n",
    "    Args:\n",
    "        index: Pinecone index\n",
    "        embeddings: array of embeddings\n",
    "        df: dataframe with data\n",
    "        mode: 'text' or 'image'\n",
    "        length: total number of records\n",
    "        batch_size: size of each batch\n",
    "    \"\"\"\n",
    "    for start_idx in range(0, length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, length)\n",
    "        batch_records = []\n",
    "        \n",
    "        for i in range(start_idx, end_idx):\n",
    "            if mode == 'text':\n",
    "                batch_records.append({\n",
    "                    \"id\": str(mode) + str(i),\n",
    "                    \"values\": embeddings[i],\n",
    "                    \"metadata\": {str(mode): df[\"Clipinfo\"].iloc[i]}\n",
    "                })\n",
    "            elif mode == 'image':\n",
    "                # Comprimir la imagen y convertir a string\n",
    "                compressed_img = compress_image(df[mode].iloc[i]['bytes'], quality=5)\n",
    "                batch_records.append({\n",
    "                    \"id\": str(mode) + str(i),\n",
    "                    \"values\": embeddings[i],\n",
    "                    \"metadata\": {str(mode): bytes_to_str(compressed_img)}\n",
    "                })\n",
    "            else:\n",
    "                raise ValueError(\"mode must be either 'text' or 'image'\")\n",
    "        \n",
    "        # Subir el batch actual\n",
    "        index.upsert(\n",
    "            vectors=batch_records,\n",
    "            namespace=\"space-\" + mode + \"-fashion\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Processed batch {start_idx//batch_size + 1} of {(length + batch_size - 1)//batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_embeddings_to_pine_cone(\n",
    "    index=index, \n",
    "    embeddings=text_embeddings, \n",
    "    df=df, \n",
    "    mode='text', \n",
    "    length=len(text_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1 of 77\n",
      "Processed batch 2 of 77\n",
      "Processed batch 3 of 77\n",
      "Processed batch 4 of 77\n",
      "Processed batch 5 of 77\n",
      "Processed batch 6 of 77\n",
      "Processed batch 7 of 77\n",
      "Processed batch 8 of 77\n",
      "Processed batch 9 of 77\n",
      "Processed batch 10 of 77\n",
      "Processed batch 11 of 77\n",
      "Processed batch 12 of 77\n",
      "Processed batch 13 of 77\n",
      "Processed batch 14 of 77\n",
      "Processed batch 15 of 77\n",
      "Processed batch 16 of 77\n",
      "Processed batch 17 of 77\n",
      "Processed batch 18 of 77\n",
      "Processed batch 19 of 77\n",
      "Processed batch 20 of 77\n",
      "Processed batch 21 of 77\n",
      "Processed batch 22 of 77\n",
      "Processed batch 23 of 77\n",
      "Processed batch 24 of 77\n",
      "Processed batch 25 of 77\n",
      "Processed batch 26 of 77\n",
      "Processed batch 27 of 77\n",
      "Processed batch 28 of 77\n",
      "Processed batch 29 of 77\n",
      "Processed batch 30 of 77\n",
      "Processed batch 31 of 77\n",
      "Processed batch 32 of 77\n",
      "Processed batch 33 of 77\n",
      "Processed batch 34 of 77\n",
      "Processed batch 35 of 77\n",
      "Processed batch 36 of 77\n",
      "Processed batch 37 of 77\n",
      "Processed batch 38 of 77\n",
      "Processed batch 39 of 77\n",
      "Processed batch 40 of 77\n",
      "Processed batch 41 of 77\n",
      "Processed batch 42 of 77\n",
      "Processed batch 43 of 77\n",
      "Processed batch 44 of 77\n",
      "Processed batch 45 of 77\n",
      "Processed batch 46 of 77\n",
      "Processed batch 47 of 77\n",
      "Processed batch 48 of 77\n",
      "Processed batch 49 of 77\n",
      "Processed batch 50 of 77\n",
      "Processed batch 51 of 77\n",
      "Processed batch 52 of 77\n",
      "Processed batch 53 of 77\n",
      "Processed batch 54 of 77\n",
      "Processed batch 55 of 77\n",
      "Processed batch 56 of 77\n",
      "Processed batch 57 of 77\n",
      "Processed batch 58 of 77\n",
      "Processed batch 59 of 77\n",
      "Processed batch 60 of 77\n",
      "Processed batch 61 of 77\n",
      "Processed batch 62 of 77\n",
      "Processed batch 63 of 77\n",
      "Processed batch 64 of 77\n",
      "Processed batch 65 of 77\n",
      "Processed batch 66 of 77\n",
      "Processed batch 67 of 77\n",
      "Processed batch 68 of 77\n",
      "Processed batch 69 of 77\n",
      "Processed batch 70 of 77\n",
      "Processed batch 71 of 77\n",
      "Processed batch 72 of 77\n",
      "Processed batch 73 of 77\n",
      "Processed batch 74 of 77\n",
      "Processed batch 75 of 77\n",
      "Processed batch 76 of 77\n",
      "Processed batch 77 of 77\n"
     ]
    }
   ],
   "source": [
    "push_embeddings_to_pine_cone(\n",
    "    index=index, \n",
    "    embeddings=image_embeddings, \n",
    "    df=df, \n",
    "    mode='image', \n",
    "    batch_size=25,\n",
    "    length=len(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
